{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00639790-3c60-4f66-9ec3-25d4ee17eaec",
      "metadata": {
        "id": "00639790-3c60-4f66-9ec3-25d4ee17eaec"
      },
      "source": [
        "# Self Driving Traffic Sign Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f876881-edf5-4099-8be5-d34a393877a8",
      "metadata": {
        "id": "8f876881-edf5-4099-8be5-d34a393877a8"
      },
      "source": [
        "Isaiah Jenkins"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0dcbf3-c303-49ec-8187-7b44990b1ece",
      "metadata": {
        "id": "bb0dcbf3-c303-49ec-8187-7b44990b1ece"
      },
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "012b37ac-9efb-417e-a169-9011d90a605a",
      "metadata": {
        "id": "012b37ac-9efb-417e-a169-9011d90a605a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f0a8fe-01b0-4928-beba-1d32baf1ece0",
      "metadata": {
        "id": "95f0a8fe-01b0-4928-beba-1d32baf1ece0"
      },
      "source": [
        "## 1. About the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244d7748-5cd9-42ef-87bb-fe8ec7e2d567",
      "metadata": {
        "id": "244d7748-5cd9-42ef-87bb-fe8ec7e2d567"
      },
      "source": [
        "1. a. Description\n",
        "\n",
        "Throughout this analysis we will explore Udacity's Self Driving dataset. This dataset consists of images for thousands of pedestrians, bikers, cars, and traffic lights. Although traffic light images are underrepresented in the dataset, the focus of the analysis will be based solely on traffic light images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574bbf2e-d81b-458b-a2b1-ad6ecf906910",
      "metadata": {
        "id": "574bbf2e-d81b-458b-a2b1-ad6ecf906910"
      },
      "source": [
        "1. b. Data dictionary, 97,942 labels across 11 classes and 15,000 images, 1,720 null examples (images with no labels).\n",
        "\n",
        "Class Balance across images (labels in each image)\n",
        "\n",
        "* car - 64,399 - over represented\n",
        "* pedestrian - 10,806\n",
        "* trafficLight-Red - 6,870\n",
        "* trafficLight-Green - 5,465 - under represented\n",
        "* truck - 3,623 - under represented\n",
        "* trafficLight - 2,568 - under represented\n",
        "* biker - 1,864 - under represented\n",
        "* trafficLight-RedLeft - 1,751 - under represented\n",
        "* trafficLight-GreenLeft - 310 - under represented\n",
        "* trafficLight-Yellow - 272 - under represented\n",
        "* trafficLight-YellowLeft - 14 - under represented"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ba6629-daed-4c51-bd9f-dcea2b5d31f1",
      "metadata": {
        "id": "a3ba6629-daed-4c51-bd9f-dcea2b5d31f1"
      },
      "source": [
        "## 2. Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "573536a5-7aa4-4a25-bcb8-b076bafa5eb3",
      "metadata": {
        "id": "573536a5-7aa4-4a25-bcb8-b076bafa5eb3"
      },
      "source": [
        "Throughout this analysis, we will explore and build various deep learning convolutional neural network (CNN) architectures to detect traffic light signs, aiming to optimize accuracy and efficiency. Our objective is to compare different model variations, such as CNNs with different depths, pre-trained models, and data augmentation techniques, to determine the most effective approach. Potential challenges include handling variations in lighting conditions, occlusions, and small object sizes, which may impact detection performance. Additionally, dataset imbalances and misclassifications due to similar-looking traffic signs could introduce biases, requiring careful preprocessing and model tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70704988-df5d-414e-bb7b-a6e2ce582de2",
      "metadata": {
        "id": "70704988-df5d-414e-bb7b-a6e2ce582de2"
      },
      "source": [
        "## 3. Data Exploration, Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb591fe-c7b6-40ae-904c-a265459646e7",
      "metadata": {
        "id": "8fb591fe-c7b6-40ae-904c-a265459646e7"
      },
      "source": [
        "* Extract traffic sign & traffic light images from the dataset to help with computational efficiency.\n",
        "* Resize images to standardize dataset making it computationally efficient making it easier to analyze data.\n",
        "* Normalize pixel values (0-1 range) to help with generalization.\n",
        "* Augment data (rotation, brightness shifts, contrast changes) to improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7ec18e-6c27-4948-b9a7-bf600e68ae78",
      "metadata": {
        "id": "4b7ec18e-6c27-4948-b9a7-bf600e68ae78"
      },
      "source": [
        "### Load in data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "!unzip -o -q '/content/gdrive/MyDrive/data.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "vx6mG2FtnLqI",
        "outputId": "34dc7e56-7e01-4b8c-fb05-06d8a85aca42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vx6mG2FtnLqI",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ca496251-ff93-4235-bffe-faef1cb98d4d",
      "metadata": {
        "id": "ca496251-ff93-4235-bffe-faef1cb98d4d"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/export'\n",
        "CSV_PATH = os.path.join(DATASET_PATH, '_annotations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2f534363-2f7b-48d3-937d-2fa7a2c45a28",
      "metadata": {
        "id": "2f534363-2f7b-48d3-937d-2fa7a2c45a28"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(CSV_PATH) # load annotations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['class'].isin(['trafficLight-Red', 'trafficLight_Yellow', 'trafficLight-Green'])]"
      ],
      "metadata": {
        "id": "k0-Aze907oI8"
      },
      "id": "k0-Aze907oI8",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ada1423d-de74-4699-9fd5-3d367983a643",
      "metadata": {
        "id": "ada1423d-de74-4699-9fd5-3d367983a643"
      },
      "outputs": [],
      "source": [
        "file_names = df['filename'].values # image files names\n",
        "labels = df['class'].values # classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "12586dcf-485e-4106-ae65-223e7f0d2998",
      "metadata": {
        "id": "12586dcf-485e-4106-ae65-223e7f0d2998"
      },
      "outputs": [],
      "source": [
        "label_map = {\"trafficLight-Red\": 0, \"trafficLight_Yellow\": 1, \"trafficLight-Green\": 2, }  # Adjust based on dataset\n",
        "labels = [label_map[label] for label in labels if label in label_map]  # Convert text labels to integers\n",
        "labels = to_categorical(labels, num_classes=3)  # Convert to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split Data into Train (80%) and Test (20%)\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "    file_names, labels, test_size=0.2, random_state=42, stratify=labels, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "t2j27DiCJ6Wo"
      },
      "id": "t2j27DiCJ6Wo",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "04f26a31-6e7e-49b1-b097-0a727da54e8b",
      "metadata": {
        "id": "04f26a31-6e7e-49b1-b097-0a727da54e8b"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7697debe-3e67-4398-9cd9-d17b10ab1368",
      "metadata": {
        "id": "7697debe-3e67-4398-9cd9-d17b10ab1368"
      },
      "outputs": [],
      "source": [
        "# Function to Load & Preprocess Images\n",
        "def load_and_preprocess_image(file_name, label):\n",
        "    img_path = os.path.join(DATASET_PATH, file_name)  # Update image folder path\n",
        "    img = load_img(img_path, target_size=IMG_SIZE)  # Load & Resize Image\n",
        "    img = img_to_array(img) / 255.0  # Convert to NumPy array & Normalize (0-1)\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c6d4c61e-4129-4780-a0fe-20b0f962c868",
      "metadata": {
        "id": "c6d4c61e-4129-4780-a0fe-20b0f962c868"
      },
      "outputs": [],
      "source": [
        "# Create Generators for Train & Test Data\n",
        "def data_generator(file_list, label_list):\n",
        "    for f, l in zip(file_list, label_list):\n",
        "        yield load_and_preprocess_image(f, l)  # Load one image at a time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "jopWUlOEL1M9"
      },
      "id": "jopWUlOEL1M9",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tf.data.Dataset that Loads Images on Demand\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_files, train_labels),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),  # Image Shape\n",
        "        tf.TensorSpec(shape=(3,), dtype=tf.float32)  # One-hot Encoded Label\n",
        "    )\n",
        ").batch(BATCH_SIZE).shuffle(1000).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(test_files, test_labels),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),  # Image Shape\n",
        "        tf.TensorSpec(shape=(3,), dtype=tf.float32)  # One-hot Encoded Label\n",
        "    )\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "d_1jzFPb0an0"
      },
      "id": "d_1jzFPb0an0",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a Small Batch\n",
        "for img_batch, label_batch in train_dataset.take(1):\n",
        "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
        "    print(f\"Label Batch Shape: {label_batch.shape}\")"
      ],
      "metadata": {
        "id": "RV0fh5wH1rcS",
        "outputId": "0464a4e0-47fa-436d-968e-2e9561b11f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RV0fh5wH1rcS",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Batch Shape: (10, 224, 224, 3)\n",
            "Label Batch Shape: (10, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be13a81-f2fd-49ef-aa7f-2d29ab5dfa78",
      "metadata": {
        "id": "9be13a81-f2fd-49ef-aa7f-2d29ab5dfa78"
      },
      "source": [
        "## 4. CNN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e001153-dad0-498b-af1a-4dffb21927d9",
      "metadata": {
        "id": "0e001153-dad0-498b-af1a-4dffb21927d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2925d0-d945-49dc-9041-23e3efad28fa",
      "metadata": {
        "id": "3e2925d0-d945-49dc-9041-23e3efad28fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c351690e-0e68-411c-8cd4-d71561b3f0d1",
      "metadata": {
        "id": "c351690e-0e68-411c-8cd4-d71561b3f0d1"
      },
      "source": [
        "### Summary of Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d38059-3f5e-451a-98de-b18b5ee0de83",
      "metadata": {
        "id": "b8d38059-3f5e-451a-98de-b18b5ee0de83"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4960148-ddd0-4559-8911-fd45133aded0",
      "metadata": {
        "id": "f4960148-ddd0-4559-8911-fd45133aded0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ff95ff82-0b8b-403f-b952-a05799e2566c",
      "metadata": {
        "id": "ff95ff82-0b8b-403f-b952-a05799e2566c"
      },
      "source": [
        "## 5. Insights and Key Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5ea98e-2357-45ed-a658-84514f93ce3b",
      "metadata": {
        "id": "cb5ea98e-2357-45ed-a658-84514f93ce3b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa028e92-5e80-4a89-9e9d-ac905adec60e",
      "metadata": {
        "id": "fa028e92-5e80-4a89-9e9d-ac905adec60e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7f70328d-c190-4a44-add0-667c97ed67d1",
      "metadata": {
        "id": "7f70328d-c190-4a44-add0-667c97ed67d1"
      },
      "source": [
        "## 6. Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c09d7c-723f-459e-9c0c-37408dcf2df4",
      "metadata": {
        "id": "08c09d7c-723f-459e-9c0c-37408dcf2df4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}