{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00639790-3c60-4f66-9ec3-25d4ee17eaec",
   "metadata": {},
   "source": [
    "# Self Driving Traffic Sign Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f876881-edf5-4099-8be5-d34a393877a8",
   "metadata": {},
   "source": [
    "Isaiah Jenkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dcbf3-c303-49ec-8187-7b44990b1ece",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b37ac-9efb-417e-a169-9011d90a605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0a8fe-01b0-4928-beba-1d32baf1ece0",
   "metadata": {},
   "source": [
    "## 1. About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d7748-5cd9-42ef-87bb-fe8ec7e2d567",
   "metadata": {},
   "source": [
    "1. a. Description\n",
    "\n",
    "Throughout this analysis we will explore Udacity's Self Driving dataset. This dataset consists of images for thousands of pedestrians, bikers, cars, and traffic lights. Although traffic light images are underrepresented in the dataset, the focus of the analysis will be based solely on traffic light images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bbf2e-d81b-458b-a2b1-ad6ecf906910",
   "metadata": {},
   "source": [
    "1. b. Data dictionary, 97,942 labels across 11 classes and 15,000 images, 1,720 null examples (images with no labels).\n",
    "\n",
    "Class Balance across images (labels in each image)\n",
    "\n",
    "* car - 64,399 - over represented\n",
    "* pedestrian - 10,806\n",
    "* trafficLight-Red - 6,870\n",
    "* trafficLight-Green - 5,465 - under represented\n",
    "* truck - 3,623 - under represented\n",
    "* trafficLight - 2,568 - under represented\n",
    "* biker - 1,864 - under represented\n",
    "* trafficLight-RedLeft - 1,751 - under represented\n",
    "* trafficLight-GreenLeft - 310 - under represented\n",
    "* trafficLight-Yellow - 272 - under represented\n",
    "* trafficLight-YellowLeft - 14 - under represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba6629-daed-4c51-bd9f-dcea2b5d31f1",
   "metadata": {},
   "source": [
    "## 2. Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573536a5-7aa4-4a25-bcb8-b076bafa5eb3",
   "metadata": {},
   "source": [
    "Throughout this analysis, we will explore and build various deep learning convolutional neural network (CNN) architectures to detect traffic light signs, aiming to optimize accuracy and efficiency. Our objective is to compare different model variations, such as CNNs with different depths, pre-trained models, and data augmentation techniques, to determine the most effective approach. Potential challenges include handling variations in lighting conditions, occlusions, and small object sizes, which may impact detection performance. Additionally, dataset imbalances and misclassifications due to similar-looking traffic signs could introduce biases, requiring careful preprocessing and model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70704988-df5d-414e-bb7b-a6e2ce582de2",
   "metadata": {},
   "source": [
    "## 3. Data Exploration, Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb591fe-c7b6-40ae-904c-a265459646e7",
   "metadata": {},
   "source": [
    "* Extract traffic sign & traffic light images from the dataset to help with computational efficiency.\n",
    "* Resize images to standardize dataset making it computationally efficient making it easier to analyze data.\n",
    "* Normalize pixel values (0-1 range) to help with generalization.\n",
    "* Augment data (rotation, brightness shifts, contrast changes) to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ec18e-6c27-4948-b9a7-bf600e68ae78",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca496251-ff93-4235-bffe-faef1cb98d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/export'\n",
    "CSV_PATH = os.path.join(DATASET_PATH, '_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f534363-2f7b-48d3-937d-2fa7a2c45a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH) # load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12586dcf-485e-4106-ae65-223e7f0d2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"trafficLight-Red\": 0, \"trafficLight-RedLeft\": 1, \"trafficLight_Yellow\": 2, \"trafficLight_YellowLeft\": 3, \"trafficLight-Green\": 4, \"trafficLight-GreenLeft\": 5 }  # Adjust based on dataset\n",
    "labels = [label_map[label] for label in labels if label in label_map]  # Convert text labels to integers\n",
    "labels = to_categorical(labels, num_classes=6)  # Convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1423d-de74-4699-9fd5-3d367983a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = df['filename'].values # image files names\n",
    "labels = df['class'].values # classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4c61e-4129-4780-a0fe-20b0f962c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Images\n",
    "image_data = [load_and_preprocess_image(f, l) for f, l in zip(filenames, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f26a31-6e7e-49b1-b097-0a727da54e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb4a01-fc25-4b4f-80c4-458dca3efb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Dataset\n",
    "images, labels = zip(*image_data)\n",
    "images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697debe-3e67-4398-9cd9-d17b10ab1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load & Preprocess Images\n",
    "def load_and_preprocess_image(file_name, label):\n",
    "    img_path = os.path.join(DATASET_PATH, \"images\", file_name)  # Update image folder path\n",
    "    img = load_img(img_path, target_size=IMG_SIZE)  # Load & Resize Image\n",
    "    img = img_to_array(img) / 255.0  # Convert to NumPy array & Normalize (0-1)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c5eef-5d56-4cfb-b00f-62435d762cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data Dataset for Training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels)).batch(32).shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be13a81-f2fd-49ef-aa7f-2d29ab5dfa78",
   "metadata": {},
   "source": [
    "## 4. CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e001153-dad0-498b-af1a-4dffb21927d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2925d0-d945-49dc-9041-23e3efad28fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c351690e-0e68-411c-8cd4-d71561b3f0d1",
   "metadata": {},
   "source": [
    "### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e24a1d-c7d7-4fc6-a8dc-ede7e8eac4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Dataset\n",
    "for img_batch, label_batch in dataset.take(1):\n",
    "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Label Batch Shape: {label_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d38059-3f5e-451a-98de-b18b5ee0de83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4960148-ddd0-4559-8911-fd45133aded0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff95ff82-0b8b-403f-b952-a05799e2566c",
   "metadata": {},
   "source": [
    "## 5. Insights and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ea98e-2357-45ed-a658-84514f93ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa028e92-5e80-4a89-9e9d-ac905adec60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f70328d-c190-4a44-add0-667c97ed67d1",
   "metadata": {},
   "source": [
    "## 6. Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c09d7c-723f-459e-9c0c-37408dcf2df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
